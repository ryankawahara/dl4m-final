{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9zzjrGKPxr6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import librosa\n",
        "import json\n",
        "import keras\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tcn"
      ],
      "metadata": {
        "id": "bzjj10WWP16x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"utils_audio_edited.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1bJGc4ADfWAhuq5ksBTKuMIgDPY5C1QSH\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import librosa\n",
        "import json\n",
        "import keras\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "# Must have mapping of id to array of genre labels\n",
        "# e.g. for id 1, [0,0,1,0,1,0]\n",
        "# with open('genres.json') as f:\n",
        "#   label_mapper = json.load(f)\n",
        "\n",
        "def load_data_first(data_home, corrupt_ids, track_ids = None):\n",
        "    \"\"\"\n",
        "    Load data from a specified music dataset and return the audio file paths and their corresponding labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_home : str\n",
        "        The root directory where the dataset is stored.\n",
        "    dataset_name : str, optional\n",
        "        The name of the dataset to load, by default 'gtzan_genre'.\n",
        "    version : str, optional\n",
        "        The version of the dataset to load, by default '1.0'.\n",
        "    track_ids : list of str, optional\n",
        "        A list of track IDs to load from the dataset, by default None. If None, all tracks in the dataset will be loaded.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    audio_file_paths : list of str\n",
        "        A list of audio file paths from the specified dataset.\n",
        "    labels : list of int\n",
        "        A list of corresponding labels for the audio files.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_home = \"/path/to/data_directory\"\n",
        "    >>> dataset_name = \"gtzan_genre\"\n",
        "    >>> version = \"mini\"\n",
        "    >>> track_ids = [\"track_1\", \"track_2\"]\n",
        "    >>> audio_file_paths, labels = load_data(data_home, dataset_name, version, track_ids)\n",
        "    \"\"\"\n",
        "\n",
        "    with open('/content/drive/My Drive/dl4m_final/trailer_dataset/genres.json') as f:\n",
        "      label_mapper = json.load(f)\n",
        "\n",
        "    audio_file_paths = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    print(data_home)\n",
        "\n",
        "    for file in os.listdir(data_home):\n",
        "      #print(file)\n",
        "      file_str = str(file)[:-4]\n",
        "      if file_str not in corrupt_ids:\n",
        "        audio_file_paths.append(file_str)\n",
        "        ids.append(file_str)\n",
        "        labels.append(label_mapper[file_str])\n",
        "\n",
        "\n",
        "    return audio_file_paths, labels, ids\n",
        "\n",
        "def load_data(data_home, track_ids = None):\n",
        "    \"\"\"\n",
        "    Load data from a specified music dataset and return the audio file paths and their corresponding labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_home : str\n",
        "        The root directory where the dataset is stored.\n",
        "    dataset_name : str, optional\n",
        "        The name of the dataset to load, by default 'gtzan_genre'.\n",
        "    version : str, optional\n",
        "        The version of the dataset to load, by default '1.0'.\n",
        "    track_ids : list of str, optional\n",
        "        A list of track IDs to load from the dataset, by default None. If None, all tracks in the dataset will be loaded.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    audio_file_paths : list of str\n",
        "        A list of audio file paths from the specified dataset.\n",
        "    labels : list of int\n",
        "        A list of corresponding labels for the audio files.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_home = \"/path/to/data_directory\"\n",
        "    >>> dataset_name = \"gtzan_genre\"\n",
        "    >>> version = \"mini\"\n",
        "    >>> track_ids = [\"track_1\", \"track_2\"]\n",
        "    >>> audio_file_paths, labels = load_data(data_home, dataset_name, version, track_ids)\n",
        "    \"\"\"\n",
        "\n",
        "    with open('/content/drive/My Drive/dl4m_final/trailer_dataset/genres.json') as f:\n",
        "      label_mapper = json.load(f)\n",
        "\n",
        "    audio_file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    track_ids = [x.decode() for x in track_ids]\n",
        "    #print(\"in load data\")\n",
        "    #print(\"load data track ids\")\n",
        "    #print(track_ids)\n",
        "\n",
        "    for file in os.listdir(data_home):\n",
        "      file_str = str(file)[2:-5]\n",
        "      #file_str = str(file)\n",
        "      #print(\"file_str:\")\n",
        "      #print(file_str)\n",
        "      if file_str in track_ids:\n",
        "        #print(\"path \",file_str)\n",
        "        #print(\"label \",label_mapper[file_str])\n",
        "        audio_file_paths.append(file)\n",
        "        labels.append(label_mapper[file_str])\n",
        "    #print(\"at end\")\n",
        "    #print(labels)\n",
        "    audio_file_paths = [x.decode() for x in audio_file_paths]\n",
        "    #print(audio_file_paths)\n",
        "    return audio_file_paths, labels\n",
        "\n",
        "def window_audio(audio, sample_rate, audio_seg_size, segments_overlap):\n",
        "    \"\"\"\n",
        "    Segment audio into windows with a specified size and overlap. Padding is added only to the\n",
        "    last window.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio : np.ndarray\n",
        "        The audio signal to be segmented.\n",
        "    sample_rate : int\n",
        "        The sampling rate of the audio signal.\n",
        "    audio_seg_size : float\n",
        "        The duration of each window in seconds.\n",
        "    segments_overlap : float\n",
        "        The duration of the overlap between consecutive windows in seconds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    audio_windows : list of np.ndarray\n",
        "        A list of windows of the audio signal.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> import librosa\n",
        "    >>> y, sr = librosa.load(librosa.ex('trumpet'))\n",
        "    >>> audio_windows = window_audio(y, sr, audio_seg_size=1, segments_overlap=0.5)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    import math\n",
        "    audio_windows = []\n",
        "    # Calculate the window size in samples\n",
        "    # x samples / second * y seconds = z samples in a given segment\n",
        "    window_size = int(sample_rate * audio_seg_size)\n",
        "\n",
        "\n",
        "    # Calculate the overlap size in samples\n",
        "    # same math as above\n",
        "    overlap_size = int(sample_rate * segments_overlap)\n",
        "\n",
        "\n",
        "    # Iterate through the audio signal, extracting windows\n",
        "    start_pos = 0\n",
        "    idx = 0\n",
        "    at_end = False\n",
        "\n",
        "    while not at_end:\n",
        "        # current window goes from start to start + size of the window\n",
        "        # If the window end is within the audio length, extract the window\n",
        "        if ((start_pos + window_size) - overlap_size) < len(audio):\n",
        "          audio_window = audio[start_pos : start_pos + window_size]\n",
        "          audio_windows.append(audio_window)\n",
        "          \n",
        "        # Padding the last window with zeros if it extends beyond the audio length\n",
        "        else:\n",
        "          audio_window = audio[start_pos:]\n",
        "          at_end = True\n",
        "          zeros_to_add = window_size - len(audio_window)\n",
        "          for _ in range(zeros_to_add):\n",
        "            audio_window = np.append(audio_window,0)\n",
        "          audio_windows.append(audio_window)\n",
        "        # Add the window to the list of audio windows\n",
        "        \n",
        "        # Update the start position for the next window, considering the overlap\n",
        "        start_pos = (start_pos + window_size) - overlap_size\n",
        "        idx += 1\n",
        "\n",
        "    #print(\"out of while loop\")\n",
        "    return(audio_windows)\n",
        "\n",
        "def compute_mel_spectrogram(audio, sample_rate=22050, n_mels=128, hop_length=512):\n",
        "    \"\"\"\n",
        "    Compute the normalized Mel spectrogram of an audio signal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio : np.ndarray\n",
        "        Input audio signal as a 1D numpy array.\n",
        "    sample_rate : int, optional\n",
        "        Sampling rate of the audio signal, by default 22050.\n",
        "    n_mels : int, optional\n",
        "        Number of Mel bands to generate, by default 128.\n",
        "    hop_length : int, optional\n",
        "        Number of samples between successive frames, by default 512.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        Mel spectrogram as a 2D numpy array.\n",
        "\n",
        "    \"\"\"\n",
        "    # Hint: use librosa melspectrogram and librosa power_to_db\n",
        "    from librosa import power_to_db\n",
        "    from librosa.feature import melspectrogram\n",
        "\n",
        "    # Compute Mel spectrogram from the audio signal\n",
        "    spectrogram = melspectrogram(y = audio, sr = sample_rate, hop_length = hop_length, n_mels = n_mels)\n",
        "    \n",
        "    # Convert the Mel spectrogram to dB scale\n",
        "    spectrogram = power_to_db(spectrogram)\n",
        "    \n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "def win_generator(data_home, augment, track_ids=None, sample_rate=22050, pitch_shift_steps=2,\n",
        "                   n_mels=128, hop_length=512, audio_seg_size=1, segments_overlap=0.5, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generator function that yields Mel spectrograms and labels from the specified dataset, with optional data augmentation. \n",
        "    Audio is broken down in small windows, the spectrogram is computed and yielded along with the label. \n",
        "    The label of the window is assumed to be the same as the label for the entire track.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_home : str\n",
        "        The root directory where the dataset is stored.\n",
        "    augment : bool\n",
        "        Whether to apply data augmentation (pitch shifting) to the audio.\n",
        "    track_ids : list of str, optional\n",
        "        A list of track IDs to load from the dataset, by default None. If None, all tracks in the dataset will be loaded.\n",
        "    sample_rate : int, optional\n",
        "        The sample rate at which to load the audio, by default 22050.\n",
        "    pitch_shift_steps : int, optional\n",
        "        The number of steps by which to shift the pitch for data augmentation, by default 2.\n",
        "    n_mels : int, optional\n",
        "        The number of Mel bands to generate, by default 128.\n",
        "    hop_length : int, optional\n",
        "        The number of samples between successive frames, by default 512.\n",
        "    audio_seg_size : float, optional\n",
        "        The size of audio segments in seconds, by default 1.\n",
        "    segments_overlap : float, optional\n",
        "        The overlap between audio segments in seconds, by default 0.5.\n",
        "    shuffle : bool, optional\n",
        "        Whether to shuffle the data before iterating, by default True.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    spectrogram : np.ndarray\n",
        "        A NumPy array containing the Mel spectrogram data.\n",
        "    label : int\n",
        "        The corresponding label for the spectrogram.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_home = \"/path/to/data_directory\"\n",
        "    >>> augment = True\n",
        "    >>> track_ids = [\"track_1\", \"track_2\"]\n",
        "    >>> generator = win_generator(data_home, augment, track_ids)\n",
        "    >>> for spectrogram, label in generator:\n",
        "    ...     # Process spectrogram and label\n",
        "    \"\"\"\n",
        "\n",
        "    # Get list of audio paths and their corresponding labels\n",
        "    #print(\"in generator\")\n",
        "    #print(label_mapper)\n",
        "    #print(\"track ids = \", track_ids)\n",
        "    audio_file_paths, labels = load_data(data_home, track_ids=track_ids)\n",
        "    #print([audio_file_paths,labels])\n",
        "\n",
        "    # Convert labels to numpy array\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Shuffle data\n",
        "    if shuffle:\n",
        "      idxs = np.random.permutation(len(labels))\n",
        "      audio_file_paths = [audio_file_paths[i] for i in idxs]\n",
        "      labels = labels[idxs]\n",
        "\n",
        "\n",
        "    for idx in range(len(audio_file_paths)):\n",
        "\n",
        "        # Load audio at given sample_rate and label\n",
        "        label = labels[idx]\n",
        "        #audio, _ = librosa.load(audio_file_paths[idx], sr=sample_rate)\n",
        "        #print(\"data_home \",data_home)\n",
        "        print(\"file path \", audio_file_paths[idx])\n",
        "        audio, _ = librosa.load(data_home.decode()+audio_file_paths[idx], sr=sample_rate)\n",
        "\n",
        "        # Shorten audio to 29s due to imprecisions in duration of GTZAN\n",
        "        # (ensures same duration files)\n",
        "        audio = audio[:29*sample_rate]\n",
        "\n",
        "        # Apply augmentation\n",
        "        if augment:\n",
        "            audio = pitch_shift_audio(audio, sample_rate, pitch_shift_steps)\n",
        "\n",
        "        # Compute audio windowing\n",
        "        audio_windows = window_audio(audio, sample_rate, audio_seg_size, segments_overlap)\n",
        "\n",
        "        # Loop over windows\n",
        "        for window in audio_windows:\n",
        "            \n",
        "            # Compute Mel spectrogram\n",
        "            spectrogram = compute_mel_spectrogram(window, sample_rate, n_mels, hop_length)\n",
        "            spectrogram = np.expand_dims(spectrogram, axis=2)\n",
        "            if spectrogram.shape != (128,44,1):\n",
        "              print(audio_file_paths[idx])\n",
        "              print(\"is a BAD ID\")\n",
        "              continue\n",
        "            #print(spectrogram)\n",
        "            #print(\"shape\")\n",
        "            #print(spectrogram.shape)\n",
        "\n",
        "            #print(\"leaving generator\")\n",
        "            yield spectrogram, label\n",
        "\n",
        "def create_dataset(data_generator, input_args, input_shape):\n",
        "    \"\"\"\n",
        "    Create a TensorFlow dataset from a data generator function along with the specified input arguments and shape.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_generator : callable\n",
        "        The data generator function to use for creating the dataset.\n",
        "    input_args : list\n",
        "        A list containing the arguments to be passed to the data generator function.\n",
        "    input_shape : tuple\n",
        "        A tuple representing the shape of the input data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset : tf.data.Dataset\n",
        "        A TensorFlow dataset created from the data generator function.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> def sample_generator():\n",
        "    ...     for i in range(10):\n",
        "    ...         yield np.random.random((4, 4)), i\n",
        "    >>> input_args = []\n",
        "    >>> input_shape = (4, 4, 1)\n",
        "    >>> dataset = create_dataset(sample_generator, input_args, input_shape)\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      data_generator,\n",
        "      args=input_args,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
        "          #tf.TensorSpec(shape=(), dtype=tf.uint8)))\n",
        "          tf.TensorSpec(shape=(10,), dtype=tf.uint8)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def wav_generator(data_home, augment, track_ids=None, sample_rate=22050, pitch_shift_steps=2, shuffle=True):\n",
        "\n",
        "    # Hint: base your generator on the win_generator\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    audio_file_paths, labels = load_data(data_home, track_ids=track_ids)\n",
        "    print([audio_file_paths,labels])\n",
        "\n",
        "    # Convert labels to numpy array\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Shuffle data\n",
        "    if shuffle:\n",
        "      idxs = np.random.permutation(len(labels))\n",
        "      audio_file_paths = [audio_file_paths[i] for i in idxs]\n",
        "      labels = labels[idxs]\n",
        "\n",
        "\n",
        "    for idx in range(len(audio_file_paths)):\n",
        "\n",
        "        # Load audio at given sample_rate and label\n",
        "        label = labels[idx]\n",
        "        #audio, _ = librosa.load(audio_file_paths[idx], sr=sample_rate)\n",
        "        #print(\"data_home \",data_home)\n",
        "        print(\"file path \", audio_file_paths[idx])\n",
        "        audio, _ = librosa.load(data_home.decode()+audio_file_paths[idx], sr=sample_rate)\n",
        "\n",
        "        # Shorten audio to 29s due to imprecisions in duration of GTZAN\n",
        "        # (ensures same duration files)\n",
        "        audio = audio[:29*sample_rate]\n",
        "\n",
        "        # Apply augmentation\n",
        "        if augment:\n",
        "            audio = pitch_shift_audio(audio, sample_rate, pitch_shift_steps)\n",
        "\n",
        "        if audio.shape != (464000,):\n",
        "          continue\n",
        "        \n",
        "        yield audio, label\n",
        "\n",
        "def extract_yamnet_embedding(wav_data, yamnet):\n",
        "    \"\"\"\n",
        "    Run YAMNet to extract embeddings from the wav data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    wav_data : np.ndarray\n",
        "        The audio signal to be processed.\n",
        "    yamnet : tensorflow.keras.Model\n",
        "        The pre-trained YAMNet model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The extracted embeddings from YAMNet.\n",
        "    \"\"\"\n",
        "    # Hint: check the tensorflow models to see how yamnet should be used\n",
        "    # YOUR CODE HERE\n",
        "    scores, embeddings, spectrogram = yamnet(wav_data)\n",
        "    return(embeddings)\n",
        "\n",
        "def reload_tcn(model_path, weights_path, optimizer, loss, metrics):\n",
        "    \"\"\"\n",
        "    Reload a TCN model from a JSON file and restore its weights. \n",
        "    Preferred method when dealing with custom layers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_path : str\n",
        "        The path to the JSON file containing the model architecture.\n",
        "    weights_path : str\n",
        "        The path to the model weights file.\n",
        "    optimizer : str or tf.keras.optimizers.Optimizer\n",
        "        The optimizer to use when compiling the model.\n",
        "    loss : str or tf.keras.losses.Loss\n",
        "        The loss function to use when compiling the model.\n",
        "    metrics : list of str or tf.keras.metrics.Metric\n",
        "        The list of metrics to use when compiling the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    reloaded_model : tf.keras.Model\n",
        "        The reloaded model with the restored weights.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> model_path = 'path/to/saved_model.json'\n",
        "    >>> weights_path = 'path/to/saved_weights.h5'\n",
        "    >>> optimizer = 'adam'\n",
        "    >>> loss = 'sparse_categorical_crossentropy'\n",
        "    >>> metrics = ['accuracy']\n",
        "    >>> reloaded_model = reload_tcn(model_path, weights_path, optimizer, loss, metrics)\n",
        "    \"\"\"\n",
        "    # Load the best checkpoint of the model from json file (due to custom layers)\n",
        "\n",
        "    from tensorflow.keras.models import model_from_json\n",
        "    \n",
        "    loaded_json = open(model_path, 'r').read()\n",
        "    reloaded_model = model_from_json(loaded_json, custom_objects={'TCN': TCN})\n",
        "\n",
        "    reloaded_model.compile(optimizer=optimizer, \n",
        "                         loss=loss, \n",
        "                       metrics=metrics)\n",
        "    # restore weights\n",
        "    reloaded_model.load_weights(weights_path)\n",
        "\n",
        "    return reloaded_model\n",
        "\n"
      ],
      "metadata": {
        "id": "2d1ftfUqctSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_home = '/content/drive/My Drive/dl4m_final/trailer_dataset/'"
      ],
      "metadata": {
        "id": "HzZVz9DPP4zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Must have id/label mapping\n",
        "# e.g. for id 1, [0,0,1,0,1,0]\n",
        "with open('genres.json') as f:\n",
        "  label_mapper = json.load(f)"
      ],
      "metadata": {
        "id": "U3RR2fZtP6Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some IDs cause model to crash - remove these\n",
        "corrupt_ids = [\"gYbW1F_c9eM\", \"VW-F1H-Nonk\", \"j9N0nvBITzk\", \"xNstK5rbzcw\", \"5tGgqyhCIXQ\", \"t2LI5OOifsQ\", \"lcwmDAYt22k\", \"RGyrxamYhUA\", \"vlEwqBrbPPU\"]"
      ],
      "metadata": {
        "id": "9NzLI0vYQuZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data, test_labels, test_ids = load_data_first(data_home+\"demo_small/audio\", corrupt_ids)"
      ],
      "metadata": {
        "id": "O2heRPCIP7Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "\n",
        "# Waveform\n",
        "sample_rate = 16000  # 16kHz for Yamnet\n",
        "augment = False\n",
        "input_shape = (29*sample_rate,)\n",
        "\n",
        "input_args_test  = [data_home+'demo_small/audio/', augment, test_ids, sample_rate]\n",
        "\n",
        "\n",
        "dataset_test= create_dataset(wav_generator, input_args_test, input_shape)\n"
      ],
      "metadata": {
        "id": "1f0K392cP8N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "id": "73qOM05qXGPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reload_tcn(model_path, weights_path, optimizer, loss, metrics):\n",
        "    \"\"\"\n",
        "    Reload a TCN model from a JSON file and restore its weights. \n",
        "    Preferred method when dealing with custom layers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_path : str\n",
        "        The path to the JSON file containing the model architecture.\n",
        "    weights_path : str\n",
        "        The path to the model weights file.\n",
        "    optimizer : str or tf.keras.optimizers.Optimizer\n",
        "        The optimizer to use when compiling the model.\n",
        "    loss : str or tf.keras.losses.Loss\n",
        "        The loss function to use when compiling the model.\n",
        "    metrics : list of str or tf.keras.metrics.Metric\n",
        "        The list of metrics to use when compiling the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    reloaded_model : tf.keras.Model\n",
        "        The reloaded model with the restored weights.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> model_path = 'path/to/saved_model.json'\n",
        "    >>> weights_path = 'path/to/saved_weights.h5'\n",
        "    >>> optimizer = 'adam'\n",
        "    >>> loss = 'sparse_categorical_crossentropy'\n",
        "    >>> metrics = ['accuracy']\n",
        "    >>> reloaded_model = reload_tcn(model_path, weights_path, optimizer, loss, metrics)\n",
        "    \"\"\"\n",
        "    # Load the best checkpoint of the model from json file (due to custom layers)\n",
        "\n",
        "    from tensorflow.keras.models import model_from_json\n",
        "    \n",
        "    loaded_json = open(model_path, 'r').read()\n",
        "    reloaded_model = model_from_json(loaded_json, custom_objects={'TCN': TCN})\n",
        "\n",
        "    reloaded_model.compile(optimizer=optimizer, \n",
        "                         loss=loss, \n",
        "                       metrics=metrics)\n",
        "    # restore weights\n",
        "    reloaded_model.load_weights(weights_path)\n",
        "\n",
        "    return reloaded_model"
      ],
      "metadata": {
        "id": "2kAdAIDFYWS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tcn import TCN"
      ],
      "metadata": {
        "id": "BgnRCD3EZxjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import model\n",
        "import tensorflow_hub as hub\n",
        "yamnet = hub.load('https://tfhub.dev/google/yamnet/1')"
      ],
      "metadata": {
        "id": "8NiITxw0aJNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping function to extract embeddings\n",
        "def map_function(audio, label):\n",
        "   return extract_yamnet_embedding(audio, yamnet), label\n",
        "   #return extract_yamnet_embedding(audio, yamnet), label\n",
        "\n",
        "# # Check input shape from example in the data\n",
        "# for e, l in dataset_train.map(map_function).take(1):\n",
        "#     print(e.shape)"
      ],
      "metadata": {
        "id": "sgmyDWUEZ40w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best checkpoint of the model \n",
        "\n",
        "# set hyperparameters\n",
        "\n",
        "optimizer = 'adam'\n",
        "# can use normal BinaryCrossentropy as well\n",
        "loss = \"BinaryCrossentropy\"\n",
        "metrics = [\"accuracy\"]\n",
        "model_path = \"yamnet_model.json\"\n",
        "model_weights = \"yamnet_weights.h5\"\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "tcn_yamnet_reloaded = reload_tcn(model_path, model_weights, optimizer, loss, metrics)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = tcn_yamnet_reloaded.predict(dataset_test.prefetch(tf.data.AUTOTUNE).map(map_function).batch(batch_size))\n",
        "#print(f\"Test Loss: {test_loss_yamnet:.4f}, Test Accuracy: {test_acc_yamnet:.4f}\")"
      ],
      "metadata": {
        "id": "gUUPht75QAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "JMPgP26SXB4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (predictions > .5)"
      ],
      "metadata": {
        "id": "kSp80xddfndd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "id": "uv66XVCHeaYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "Yn11RrvYfF-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "genre_names = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Horror', 'Mystery', 'Romance', 'Science Fiction', 'Thriller']\n",
        "cr = classification_report(y_true = test_labels, y_pred = y_pred, target_names= genre_names, output_dict = True)"
      ],
      "metadata": {
        "id": "6O7Jq_AAf4Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True)\n",
        "heatmap.set_title(\"Demo Classification Report (Decision Threshold = 50%)\")\n",
        "fig = heatmap.get_figure()\n",
        "#fig.savefig(\"demo_heatmap.png\")"
      ],
      "metadata": {
        "id": "c058m0angntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIr5XLGeg4g7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}